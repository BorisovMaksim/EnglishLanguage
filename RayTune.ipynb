{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09635542-2aef-49dd-bb50-ba62f38c58ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "\n",
    "from datasets import load_dataset, load_metric, Dataset\n",
    "from transformers import (AutoModelForSequenceClassification, AutoTokenizer,\n",
    "                          Trainer, TrainingArguments)\n",
    "\n",
    "import numpy as np\n",
    "from typing import Dict\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import ray\n",
    "from ray.train.torch import TorchTrainer\n",
    "from ray.train import RunConfig, ScalingConfig, CheckpointConfig\n",
    "import ray.data\n",
    "import ray.train\n",
    "from ray.train.huggingface.transformers import prepare_trainer, RayTrainReportCallback\n",
    "from ray import tune\n",
    "from ray.tune import Tuner\n",
    "from ray.tune.schedulers.async_hyperband import ASHAScheduler\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c0d112-aa2c-4d09-a541-e23f76bdb181",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "feff819c-71c4-4a93-8d7c-0ebc86ed7ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"./data/train\"\n",
    "valid_path = \"./data/dev\"\n",
    "\n",
    "num_labels = 3\n",
    "metric_name = \"accuracy\"\n",
    "\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "validation_key = \"validation\"\n",
    "task = \"cola\"\n",
    "\n",
    "name = f\"{model_name}-finetuned-{task}\"\n",
    "actual_task = task\n",
    "batch_size = 16\n",
    "num_workers = 1\n",
    "use_gpu = True\n",
    "\n",
    "tune_epochs = 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2415e9-5948-4ac3-835a-da6d85929e67",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77d028ec-bfc5-43d9-83f6-05f7d67efa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_path + \".input0\", 'r') as f,  open(train_path + \".label\", 'r') as g:\n",
    "    train_texts = f.read().split(\"\\n\")[:-1]\n",
    "    train_labels = g.read().split(\"\\n\")[:-1]\n",
    "    \n",
    "with open(valid_path + \".input0\", 'r') as f,  open(valid_path + \".label\", 'r') as g:\n",
    "    valid_texts = f.read().split(\"\\n\")[:-1]\n",
    "    valid_labels = g.read().split(\"\\n\")[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7acdedb-3854-44cb-8f1c-bcc5e53cf324",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame([train_texts, train_labels]).T\n",
    "df_train.columns = [\"text\", 'label']\n",
    "df_train['label'] = df_train['label'].astype(int)\n",
    "train_dataset = Dataset.from_pandas(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adab1ff5-2a71-46b6-a4ab-77a850a9221d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 29412\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33c10746-fef9-44a1-9915-693121040346",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame([valid_texts, valid_labels]).T\n",
    "df_test.columns = [\"text\", 'label']\n",
    "df_test['label'] = df_test['label'].astype(int)\n",
    "test_dataset = Dataset.from_pandas(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "709cc9c6-21c7-4b4b-9c78-ab2f6f11cf63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 7353\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344eeda0-6ef4-4da3-b3cc-ddd5e569b4e3",
   "metadata": {},
   "source": [
    "# Ray training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43def4af-ad54-42e3-b07c-707f6a928598",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-25 18:26:01,394\tINFO worker.py:1724 -- Started a local Ray instance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': MaterializedDataset(\n",
       "    num_blocks=1,\n",
       "    num_rows=29412,\n",
       "    schema={text: string, label: int64}\n",
       " ),\n",
       " 'validation': MaterializedDataset(\n",
       "    num_blocks=1,\n",
       "    num_rows=7353,\n",
       "    schema={text: string, label: int64}\n",
       " )}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray_datasets = {\n",
    "    \"train\": ray.data.from_huggingface(train_dataset),\n",
    "    \"validation\": ray.data.from_huggingface(test_dataset),\n",
    "}\n",
    "ray_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "916de13a-5785-4cbc-9ad6-4baae060e4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenize input sentences\n",
    "def collate_fn(examples: Dict[str, np.array]):\n",
    "    outputs =  tokenizer(list(examples['text']), truncation=True,  \n",
    "                         padding=\"longest\",\n",
    "                         return_tensors=\"pt\",)\n",
    "\n",
    "    outputs[\"labels\"] = torch.LongTensor(examples[\"label\"])\n",
    "\n",
    "    # Move all input tensors to GPU\n",
    "    for key, value in outputs.items():\n",
    "        outputs[key] = value.cuda()\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22908eba-e20b-42fc-821e-652ef8c8cbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "\n",
    "# Calculate the maximum steps per epoch based on the number of rows in the training dataset.\n",
    "# Make sure to scale by the total number of training workers and the per device batch size.\n",
    "max_steps_per_epoch = ray_datasets[\"train\"].count() // (batch_size * num_workers)\n",
    "\n",
    "\n",
    "def train_func(config):\n",
    "    print(f\"Is CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "    metric = load_metric(\"glue\", actual_task)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_checkpoint, num_labels=num_labels\n",
    "    )\n",
    "\n",
    "    train_ds = ray.train.get_dataset_shard(\"train\")\n",
    "    eval_ds = ray.train.get_dataset_shard(\"eval\")\n",
    "\n",
    "    train_ds_iterable = train_ds.iter_torch_batches(\n",
    "        batch_size=batch_size, collate_fn=collate_fn\n",
    "    )\n",
    "    eval_ds_iterable = eval_ds.iter_torch_batches(\n",
    "        batch_size=batch_size, collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    print(\"max_steps_per_epoch: \", max_steps_per_epoch)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        name,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        learning_rate=config.get(\"learning_rate\", 2e-5),\n",
    "        num_train_epochs=config.get(\"epochs\", 2),\n",
    "        weight_decay=config.get(\"weight_decay\", 0.01),\n",
    "        push_to_hub=False,\n",
    "        max_steps=max_steps_per_epoch * config.get(\"epochs\", 2),\n",
    "        disable_tqdm=True,  # declutter the output a little\n",
    "        no_cuda=not use_gpu,  # you need to explicitly set no_cuda if you want CPUs\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        if task != \"stsb\":\n",
    "            predictions = np.argmax(predictions, axis=1)\n",
    "        else:\n",
    "            predictions = predictions[:, 0]\n",
    "        return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        args,\n",
    "        train_dataset=train_ds_iterable,\n",
    "        eval_dataset=eval_ds_iterable,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.add_callback(RayTrainReportCallback())\n",
    "\n",
    "    trainer = prepare_trainer(trainer)\n",
    "\n",
    "    print(\"Starting training\")\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b8487be-7608-4a50-81b4-90fe5fc8d0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_func,\n",
    "    scaling_config=ScalingConfig(num_workers=num_workers, use_gpu=use_gpu),\n",
    "    datasets={\n",
    "        \"train\": ray_datasets[\"train\"],\n",
    "        \"eval\": ray_datasets[\"validation\"],\n",
    "    },\n",
    "    run_config=RunConfig(\n",
    "        checkpoint_config=CheckpointConfig(\n",
    "            num_to_keep=1,\n",
    "            checkpoint_score_attribute=\"eval_loss\",\n",
    "            checkpoint_score_order=\"min\",\n",
    "        ),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4327a7f2-0366-4114-9fd4-bdc43d36d9bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-12-25 18:32:09</td></tr>\n",
       "<tr><td>Running for: </td><td>00:06:04.37        </td></tr>\n",
       "<tr><td>Memory:      </td><td>21.0/393.5 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 1.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:V100S)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status    </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  loss</th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">  epoch</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_10d8a_00000</td><td>TERMINATED</td><td>192.168.0.2:92340</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         359.401</td><td style=\"text-align: right;\">0.6566</td><td style=\"text-align: right;\">              0</td><td style=\"text-align: right;\">    1.5</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TorchTrainer pid=92340)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=92340)\u001b[0m - (ip=192.168.0.2, pid=92417) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=92417)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=92417)\u001b[0m Is CUDA available: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=92417)\u001b[0m /tmp/ipykernel_91028/683511016.py:11: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "\u001b[36m(SplitCoordinator pid=92472)\u001b[0m Auto configuring locality_with_output=['1a66715af743dabd9b80b3a893f9972e0380dae976dd72512d19bc2a']\n",
      "\u001b[36m(RayTrainWorker pid=92417)\u001b[0m /home/itmo/miniconda3/envs/automl/lib/python3.9/site-packages/datasets/load.py:752: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.0/metrics/glue/glue.py\n",
      "\u001b[36m(RayTrainWorker pid=92417)\u001b[0m You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "\u001b[36m(RayTrainWorker pid=92417)\u001b[0m Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "\u001b[36m(RayTrainWorker pid=92417)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=92417)\u001b[0m max_steps_per_epoch:  1838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=92417)\u001b[0m Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\n",
      "\u001b[36m(RayTrainWorker pid=92417)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[36m(RayTrainWorker pid=92417)\u001b[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=92417)\u001b[0m Starting training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=92417)\u001b[0m /tmp/ipykernel_91028/1795508781.py:7: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "\u001b[36m(SplitCoordinator pid=92472)\u001b[0m Executing DAG InputDataBuffer[Input] -> OutputSplitter[split(1, equal=True)]\n",
      "\u001b[36m(SplitCoordinator pid=92472)\u001b[0m Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=1.0, gpu=1.0, object_store_memory=0.0), locality_with_output=['1a66715af743dabd9b80b3a893f9972e0380dae976dd72512d19bc2a'], preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "\u001b[36m(SplitCoordinator pid=92472)\u001b[0m Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=92472) Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=92417)\u001b[0m [W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=92417)\u001b[0m {'loss': 0.7774, 'learning_rate': 9.99455930359086e-06, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SplitCoordinator pid=92473)\u001b[0m Auto configuring locality_with_output=['1a66715af743dabd9b80b3a893f9972e0380dae976dd72512d19bc2a']\n",
      "\u001b[36m(SplitCoordinator pid=92473)\u001b[0m Executing DAG InputDataBuffer[Input] -> OutputSplitter[split(1, equal=True)]\n",
      "\u001b[36m(SplitCoordinator pid=92473)\u001b[0m Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=1.0, gpu=1.0, object_store_memory=0.0), locality_with_output=['1a66715af743dabd9b80b3a893f9972e0380dae976dd72512d19bc2a'], preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "\u001b[36m(SplitCoordinator pid=92473)\u001b[0m Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=92473) Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=92417)\u001b[0m {'eval_loss': 0.7320316433906555, 'eval_matthews_correlation': 0.39615303511604993, 'eval_runtime': 13.5295, 'eval_samples_per_second': 543.479, 'eval_steps_per_second': 34.0, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=92417)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/itmo/ray_results/TorchTrainer_2023-12-25_18-26-05/TorchTrainer_10d8a_00000_0_2023-12-25_18-26-05/checkpoint_000000)\n",
      "\u001b[36m(SplitCoordinator pid=92472)\u001b[0m Executing DAG InputDataBuffer[Input] -> OutputSplitter[split(1, equal=True)]\n",
      "\u001b[36m(SplitCoordinator pid=92472)\u001b[0m Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=1.0, gpu=1.0, object_store_memory=0.0), locality_with_output=['1a66715af743dabd9b80b3a893f9972e0380dae976dd72512d19bc2a'], preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "\u001b[36m(SplitCoordinator pid=92472)\u001b[0m Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=92472) Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=92417)\u001b[0m {'loss': 0.6566, 'learning_rate': 0.0, 'epoch': 1.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SplitCoordinator pid=92473)\u001b[0m Executing DAG InputDataBuffer[Input] -> OutputSplitter[split(1, equal=True)]\n",
      "\u001b[36m(SplitCoordinator pid=92473)\u001b[0m Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=1.0, gpu=1.0, object_store_memory=0.0), locality_with_output=['1a66715af743dabd9b80b3a893f9972e0380dae976dd72512d19bc2a'], preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "\u001b[36m(SplitCoordinator pid=92473)\u001b[0m Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=92473) Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=92417)\u001b[0m {'eval_loss': 0.7379251718521118, 'eval_matthews_correlation': 0.4091076801455507, 'eval_runtime': 13.4609, 'eval_samples_per_second': 546.25, 'eval_steps_per_second': 34.173, 'epoch': 1.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=92417)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/itmo/ray_results/TorchTrainer_2023-12-25_18-26-05/TorchTrainer_10d8a_00000_0_2023-12-25_18-26-05/checkpoint_000001)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=92417)\u001b[0m {'train_runtime': 352.7186, 'train_samples_per_second': 166.75, 'train_steps_per_second': 10.422, 'train_loss': 0.7170052844889145, 'epoch': 1.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-25 18:32:09,928\tINFO tune.py:1042 -- Total run time: 364.40 seconds (364.37 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a50b3a-f978-45b3-ac89-9401f80954da",
   "metadata": {},
   "source": [
    "# Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfef6fd8-a034-428c-b696-ad71810e18a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-25 18:32:09,938\tINFO tuner_internal.py:401 -- A `RunConfig` was passed to both the `Tuner` and the `TorchTrainer`. The run config passed to the `Tuner` is the one that will be used.\n"
     ]
    }
   ],
   "source": [
    "tuner = Tuner(\n",
    "    trainer,\n",
    "    param_space={\n",
    "        \"train_loop_config\": {\n",
    "            \"learning_rate\": tune.grid_search([2e-5, 2e-4, 2e-3, 2e-2]),\n",
    "            \"epochs\": tune_epochs,\n",
    "        }\n",
    "    },\n",
    "    tune_config=tune.TuneConfig(\n",
    "        metric=\"eval_loss\",\n",
    "        mode=\"min\",\n",
    "        num_samples=1,\n",
    "        scheduler=ASHAScheduler(\n",
    "            max_t=tune_epochs,\n",
    "        ),\n",
    "    ),\n",
    "    run_config=RunConfig(\n",
    "        name=\"tune_transformers\",\n",
    "        checkpoint_config=CheckpointConfig(\n",
    "            num_to_keep=1,\n",
    "            checkpoint_score_attribute=\"eval_loss\",\n",
    "            checkpoint_score_order=\"min\",\n",
    "        ),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "978ecdb3-824a-4690-bd6b-554b449d708c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-12-25 18:53:23</td></tr>\n",
       "<tr><td>Running for: </td><td>00:21:13.89        </td></tr>\n",
       "<tr><td>Memory:      </td><td>20.9/393.5 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=4<br>Bracket: Iter 4.000: -0.9307839870452881 | Iter 1.000: -0.8592206984758377<br>Logical resource usage: 1.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:V100S)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status    </th><th>loc               </th><th style=\"text-align: right;\">       train_loop_config/le\n",
       "arning_rate</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  loss</th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">  epoch</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_ea11d_00000</td><td>TERMINATED</td><td>192.168.0.2:97812 </td><td style=\"text-align: right;\">2e-05 </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         712.982</td><td style=\"text-align: right;\">0.4253</td><td style=\"text-align: right;\">    0          </td><td style=\"text-align: right;\">   3.25</td></tr>\n",
       "<tr><td>TorchTrainer_ea11d_00001</td><td>TERMINATED</td><td>192.168.0.2:108568</td><td style=\"text-align: right;\">0.0002</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         183.064</td><td style=\"text-align: right;\">0.9574</td><td style=\"text-align: right;\">    0.000149973</td><td style=\"text-align: right;\">   0.25</td></tr>\n",
       "<tr><td>TorchTrainer_ea11d_00002</td><td>TERMINATED</td><td>192.168.0.2:111376</td><td style=\"text-align: right;\">0.002 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         182.321</td><td style=\"text-align: right;\">0.9892</td><td style=\"text-align: right;\">    0.00149973 </td><td style=\"text-align: right;\">   0.25</td></tr>\n",
       "<tr><td>TorchTrainer_ea11d_00003</td><td>TERMINATED</td><td>192.168.0.2:114275</td><td style=\"text-align: right;\">0.02  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         180.285</td><td style=\"text-align: right;\">1.1552</td><td style=\"text-align: right;\">    0.0149973  </td><td style=\"text-align: right;\">   0.25</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TorchTrainer pid=97812)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=97812)\u001b[0m - (ip=192.168.0.2, pid=97865) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=97865)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(RayTrainWorker pid=97865)\u001b[0m /tmp/ipykernel_91028/683511016.py:11: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "\u001b[36m(SplitCoordinator pid=97919)\u001b[0m Auto configuring locality_with_output=['1a66715af743dabd9b80b3a893f9972e0380dae976dd72512d19bc2a']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=97865)\u001b[0m Is CUDA available: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=97865)\u001b[0m /home/itmo/miniconda3/envs/automl/lib/python3.9/site-packages/datasets/load.py:752: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.0/metrics/glue/glue.py\n",
      "\u001b[36m(RayTrainWorker pid=97865)\u001b[0m You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "\u001b[36m(RayTrainWorker pid=97865)\u001b[0m Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "\u001b[36m(RayTrainWorker pid=97865)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(RayTrainWorker pid=97865)\u001b[0m Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "\u001b[36m(RayTrainWorker pid=97865)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[36m(RayTrainWorker pid=97865)\u001b[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=97865)\u001b[0m max_steps_per_epoch:  1838\n",
      "\u001b[36m(RayTrainWorker pid=97865)\u001b[0m Starting training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=97865)\u001b[0m /tmp/ipykernel_91028/1795508781.py:7: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=97919) Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SplitCoordinator pid=97919)\u001b[0m Executing DAG InputDataBuffer[Input] -> OutputSplitter[split(1, equal=True)]\n",
      "\u001b[36m(SplitCoordinator pid=97919)\u001b[0m Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=1.0, gpu=1.0, object_store_memory=0.0), locality_with_output=['1a66715af743dabd9b80b3a893f9972e0380dae976dd72512d19bc2a'], preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "\u001b[36m(SplitCoordinator pid=97919)\u001b[0m Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n",
      "\u001b[36m(RayTrainWorker pid=97865)\u001b[0m [W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=97865)\u001b[0m {'loss': 0.782, 'learning_rate': 1.4997279651795431e-05, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SplitCoordinator pid=97920)\u001b[0m Auto configuring locality_with_output=['1a66715af743dabd9b80b3a893f9972e0380dae976dd72512d19bc2a']\n",
      "\u001b[36m(SplitCoordinator pid=97920)\u001b[0m Executing DAG InputDataBuffer[Input] -> OutputSplitter[split(1, equal=True)]\n",
      "\u001b[36m(SplitCoordinator pid=97920)\u001b[0m Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=1.0, gpu=1.0, object_store_memory=0.0), locality_with_output=['1a66715af743dabd9b80b3a893f9972e0380dae976dd72512d19bc2a'], preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "\u001b[36m(SplitCoordinator pid=97920)\u001b[0m Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=97920) Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=97865)\u001b[0m {'eval_loss': 0.7412927746772766, 'eval_matthews_correlation': 0.39369773685844595, 'eval_runtime': 13.4839, 'eval_samples_per_second': 545.315, 'eval_steps_per_second': 34.115, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=97865)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/itmo/ray_results/tune_transformers/TorchTrainer_ea11d_00000_0_learning_rate=0.0000_2023-12-25_18-32-09/checkpoint_000000)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=97919) Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SplitCoordinator pid=97919)\u001b[0m Executing DAG InputDataBuffer[Input] -> OutputSplitter[split(1, equal=True)]\n",
      "\u001b[36m(SplitCoordinator pid=97919)\u001b[0m Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=1.0, gpu=1.0, object_store_memory=0.0), locality_with_output=['1a66715af743dabd9b80b3a893f9972e0380dae976dd72512d19bc2a'], preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "\u001b[36m(SplitCoordinator pid=97919)\u001b[0m Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=97865)\u001b[0m {'loss': 0.658, 'learning_rate': 9.99455930359086e-06, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SplitCoordinator pid=97920)\u001b[0m Executing DAG InputDataBuffer[Input] -> OutputSplitter[split(1, equal=True)]\n",
      "\u001b[36m(SplitCoordinator pid=97920)\u001b[0m Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=1.0, gpu=1.0, object_store_memory=0.0), locality_with_output=['1a66715af743dabd9b80b3a893f9972e0380dae976dd72512d19bc2a'], preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "\u001b[36m(SplitCoordinator pid=97920)\u001b[0m Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=97920) Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=97865)\u001b[0m {'eval_loss': 0.7539792060852051, 'eval_matthews_correlation': 0.406581779908781, 'eval_runtime': 13.5197, 'eval_samples_per_second': 543.873, 'eval_steps_per_second': 34.024, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=97865)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/itmo/ray_results/tune_transformers/TorchTrainer_ea11d_00000_0_learning_rate=0.0000_2023-12-25_18-32-09/checkpoint_000001)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=97919) Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SplitCoordinator pid=97919)\u001b[0m Executing DAG InputDataBuffer[Input] -> OutputSplitter[split(1, equal=True)]\n",
      "\u001b[36m(SplitCoordinator pid=97919)\u001b[0m Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=1.0, gpu=1.0, object_store_memory=0.0), locality_with_output=['1a66715af743dabd9b80b3a893f9972e0380dae976dd72512d19bc2a'], preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "\u001b[36m(SplitCoordinator pid=97919)\u001b[0m Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=97865)\u001b[0m {'loss': 0.5275, 'learning_rate': 4.99183895538629e-06, 'epoch': 2.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SplitCoordinator pid=97920)\u001b[0m Executing DAG InputDataBuffer[Input] -> OutputSplitter[split(1, equal=True)]\n",
      "\u001b[36m(SplitCoordinator pid=97920)\u001b[0m Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=1.0, gpu=1.0, object_store_memory=0.0), locality_with_output=['1a66715af743dabd9b80b3a893f9972e0380dae976dd72512d19bc2a'], preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "\u001b[36m(SplitCoordinator pid=97920)\u001b[0m Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=97920) Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=97865)\u001b[0m {'eval_loss': 0.8132387399673462, 'eval_matthews_correlation': 0.3989952703856063, 'eval_runtime': 13.4843, 'eval_samples_per_second': 545.302, 'eval_steps_per_second': 34.114, 'epoch': 2.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=97865)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/itmo/ray_results/tune_transformers/TorchTrainer_ea11d_00000_0_learning_rate=0.0000_2023-12-25_18-32-09/checkpoint_000002)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=97919) Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SplitCoordinator pid=97919)\u001b[0m Executing DAG InputDataBuffer[Input] -> OutputSplitter[split(1, equal=True)]\n",
      "\u001b[36m(SplitCoordinator pid=97919)\u001b[0m Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=1.0, gpu=1.0, object_store_memory=0.0), locality_with_output=['1a66715af743dabd9b80b3a893f9972e0380dae976dd72512d19bc2a'], preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "\u001b[36m(SplitCoordinator pid=97919)\u001b[0m Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=97865)\u001b[0m {'loss': 0.4253, 'learning_rate': 0.0, 'epoch': 3.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SplitCoordinator pid=97920)\u001b[0m Executing DAG InputDataBuffer[Input] -> OutputSplitter[split(1, equal=True)]\n",
      "\u001b[36m(SplitCoordinator pid=97920)\u001b[0m Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=1.0, gpu=1.0, object_store_memory=0.0), locality_with_output=['1a66715af743dabd9b80b3a893f9972e0380dae976dd72512d19bc2a'], preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "\u001b[36m(SplitCoordinator pid=97920)\u001b[0m Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=97920) Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=97865)\u001b[0m {'eval_loss': 0.9307839870452881, 'eval_matthews_correlation': 0.38854276596384485, 'eval_runtime': 13.4802, 'eval_samples_per_second': 545.468, 'eval_steps_per_second': 34.124, 'epoch': 3.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=97865)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/itmo/ray_results/tune_transformers/TorchTrainer_ea11d_00000_0_learning_rate=0.0000_2023-12-25_18-32-09/checkpoint_000003)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=97865)\u001b[0m {'train_runtime': 706.4031, 'train_samples_per_second': 166.522, 'train_steps_per_second': 10.408, 'train_loss': 0.5983102858650282, 'epoch': 3.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TorchTrainer pid=108568)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=108568)\u001b[0m - (ip=192.168.0.2, pid=108623) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=108623)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=108623)\u001b[0m Is CUDA available: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=108623)\u001b[0m /tmp/ipykernel_91028/683511016.py:11: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "\u001b[36m(SplitCoordinator pid=108677)\u001b[0m Auto configuring locality_with_output=['1a66715af743dabd9b80b3a893f9972e0380dae976dd72512d19bc2a']\n",
      "\u001b[36m(RayTrainWorker pid=108623)\u001b[0m /home/itmo/miniconda3/envs/automl/lib/python3.9/site-packages/datasets/load.py:752: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.0/metrics/glue/glue.py\n",
      "\u001b[36m(RayTrainWorker pid=108623)\u001b[0m You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "\u001b[36m(RayTrainWorker pid=108623)\u001b[0m Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "\u001b[36m(RayTrainWorker pid=108623)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=108623)\u001b[0m max_steps_per_epoch:  1838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=108623)\u001b[0m Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n",
      "\u001b[36m(RayTrainWorker pid=108623)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[36m(RayTrainWorker pid=108623)\u001b[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=108623)\u001b[0m Starting training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=108676) Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SplitCoordinator pid=108676)\u001b[0m Executing DAG InputDataBuffer[Input] -> OutputSplitter[split(1, equal=True)]\n",
      "\u001b[36m(SplitCoordinator pid=108676)\u001b[0m Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=1.0, gpu=1.0, object_store_memory=0.0), locality_with_output=['1a66715af743dabd9b80b3a893f9972e0380dae976dd72512d19bc2a'], preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "\u001b[36m(SplitCoordinator pid=108676)\u001b[0m Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n",
      "\u001b[36m(RayTrainWorker pid=108623)\u001b[0m /tmp/ipykernel_91028/1795508781.py:7: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "\u001b[36m(RayTrainWorker pid=108623)\u001b[0m [W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=108623)\u001b[0m {'loss': 0.9574, 'learning_rate': 0.0001499727965179543, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SplitCoordinator pid=108676)\u001b[0m Auto configuring locality_with_output=['1a66715af743dabd9b80b3a893f9972e0380dae976dd72512d19bc2a']\n",
      "\u001b[36m(SplitCoordinator pid=108677)\u001b[0m Executing DAG InputDataBuffer[Input] -> OutputSplitter[split(1, equal=True)]\n",
      "\u001b[36m(SplitCoordinator pid=108677)\u001b[0m Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=1.0, gpu=1.0, object_store_memory=0.0), locality_with_output=['1a66715af743dabd9b80b3a893f9972e0380dae976dd72512d19bc2a'], preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "\u001b[36m(SplitCoordinator pid=108677)\u001b[0m Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=108677) Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=108623)\u001b[0m {'eval_loss': 0.8985300064086914, 'eval_matthews_correlation': 0.28063325591945343, 'eval_runtime': 13.3326, 'eval_samples_per_second': 551.506, 'eval_steps_per_second': 34.502, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=108623)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/itmo/ray_results/tune_transformers/TorchTrainer_ea11d_00001_1_learning_rate=0.0002_2023-12-25_18-32-09/checkpoint_000000)\n",
      "\u001b[36m(TorchTrainer pid=111376)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=111376)\u001b[0m - (ip=192.168.0.2, pid=111430) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=111430)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=111430)\u001b[0m Is CUDA available: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=111430)\u001b[0m /tmp/ipykernel_91028/683511016.py:11: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "\u001b[36m(SplitCoordinator pid=111485)\u001b[0m Auto configuring locality_with_output=['1a66715af743dabd9b80b3a893f9972e0380dae976dd72512d19bc2a']\n",
      "\u001b[36m(RayTrainWorker pid=111430)\u001b[0m /home/itmo/miniconda3/envs/automl/lib/python3.9/site-packages/datasets/load.py:752: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.0/metrics/glue/glue.py\n",
      "\u001b[36m(RayTrainWorker pid=111430)\u001b[0m You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "\u001b[36m(RayTrainWorker pid=111430)\u001b[0m Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "\u001b[36m(RayTrainWorker pid=111430)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=111430)\u001b[0m max_steps_per_epoch:  1838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=111430)\u001b[0m Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "\u001b[36m(RayTrainWorker pid=111430)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[36m(RayTrainWorker pid=111430)\u001b[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=111430)\u001b[0m Starting training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "881407aaf40241fbb1d6b330993e88df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=111485) Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SplitCoordinator pid=111485)\u001b[0m Executing DAG InputDataBuffer[Input] -> OutputSplitter[split(1, equal=True)]\n",
      "\u001b[36m(SplitCoordinator pid=111485)\u001b[0m Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=1.0, gpu=1.0, object_store_memory=0.0), locality_with_output=['1a66715af743dabd9b80b3a893f9972e0380dae976dd72512d19bc2a'], preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "\u001b[36m(SplitCoordinator pid=111485)\u001b[0m Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n",
      "\u001b[36m(RayTrainWorker pid=111430)\u001b[0m /tmp/ipykernel_91028/1795508781.py:7: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "\u001b[36m(RayTrainWorker pid=111430)\u001b[0m [W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=111430)\u001b[0m {'loss': 0.9892, 'learning_rate': 0.001499727965179543, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SplitCoordinator pid=111486)\u001b[0m Auto configuring locality_with_output=['1a66715af743dabd9b80b3a893f9972e0380dae976dd72512d19bc2a']\n",
      "\u001b[36m(SplitCoordinator pid=111486)\u001b[0m Executing DAG InputDataBuffer[Input] -> OutputSplitter[split(1, equal=True)]\n",
      "\u001b[36m(SplitCoordinator pid=111486)\u001b[0m Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=1.0, gpu=1.0, object_store_memory=0.0), locality_with_output=['1a66715af743dabd9b80b3a893f9972e0380dae976dd72512d19bc2a'], preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "\u001b[36m(SplitCoordinator pid=111486)\u001b[0m Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "325700be75794518a10795a2cb2692c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=111486) Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=111430)\u001b[0m {'eval_loss': 0.9776955842971802, 'eval_matthews_correlation': 0.0, 'eval_runtime': 13.1467, 'eval_samples_per_second': 559.304, 'eval_steps_per_second': 34.99, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=111430)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/itmo/ray_results/tune_transformers/TorchTrainer_ea11d_00002_2_learning_rate=0.0020_2023-12-25_18-32-09/checkpoint_000000)\n",
      "\u001b[36m(TorchTrainer pid=114275)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=114275)\u001b[0m - (ip=192.168.0.2, pid=114330) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=114330)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(RayTrainWorker pid=114330)\u001b[0m /tmp/ipykernel_91028/683511016.py:11: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=114330)\u001b[0m Is CUDA available: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SplitCoordinator pid=114385)\u001b[0m Auto configuring locality_with_output=['1a66715af743dabd9b80b3a893f9972e0380dae976dd72512d19bc2a']\n",
      "\u001b[36m(RayTrainWorker pid=114330)\u001b[0m /home/itmo/miniconda3/envs/automl/lib/python3.9/site-packages/datasets/load.py:752: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.0/metrics/glue/glue.py\n",
      "\u001b[36m(RayTrainWorker pid=114330)\u001b[0m You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "\u001b[36m(RayTrainWorker pid=114330)\u001b[0m Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "\u001b[36m(RayTrainWorker pid=114330)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(RayTrainWorker pid=114330)\u001b[0m Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "\u001b[36m(RayTrainWorker pid=114330)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[36m(RayTrainWorker pid=114330)\u001b[0m Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=114330)\u001b[0m max_steps_per_epoch:  1838\n",
      "\u001b[36m(RayTrainWorker pid=114330)\u001b[0m Starting training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=114384) Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SplitCoordinator pid=114384)\u001b[0m Executing DAG InputDataBuffer[Input] -> OutputSplitter[split(1, equal=True)]\n",
      "\u001b[36m(SplitCoordinator pid=114384)\u001b[0m Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=1.0, gpu=1.0, object_store_memory=0.0), locality_with_output=['1a66715af743dabd9b80b3a893f9972e0380dae976dd72512d19bc2a'], preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "\u001b[36m(SplitCoordinator pid=114384)\u001b[0m Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n",
      "\u001b[36m(RayTrainWorker pid=114330)\u001b[0m /tmp/ipykernel_91028/1795508781.py:7: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "\u001b[36m(RayTrainWorker pid=114330)\u001b[0m [W reducer.cpp:1346] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=114330)\u001b[0m {'loss': 1.1552, 'learning_rate': 0.01499727965179543, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SplitCoordinator pid=114384)\u001b[0m Auto configuring locality_with_output=['1a66715af743dabd9b80b3a893f9972e0380dae976dd72512d19bc2a']\n",
      "\u001b[36m(SplitCoordinator pid=114385)\u001b[0m Executing DAG InputDataBuffer[Input] -> OutputSplitter[split(1, equal=True)]\n",
      "\u001b[36m(SplitCoordinator pid=114385)\u001b[0m Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=1.0, gpu=1.0, object_store_memory=0.0), locality_with_output=['1a66715af743dabd9b80b3a893f9972e0380dae976dd72512d19bc2a'], preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "\u001b[36m(SplitCoordinator pid=114385)\u001b[0m Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(pid=114385) Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=114330)\u001b[0m {'eval_loss': 0.9774772524833679, 'eval_matthews_correlation': 0.0, 'eval_runtime': 13.1348, 'eval_samples_per_second': 559.81, 'eval_steps_per_second': 35.021, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-25 18:53:23,875\tINFO tune.py:1042 -- Total run time: 1273.91 seconds (1273.89 seconds for the tuning loop).\n",
      "\u001b[36m(RayTrainWorker pid=114330)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/itmo/ray_results/tune_transformers/TorchTrainer_ea11d_00003_3_learning_rate=0.0200_2023-12-25_18-32-09/checkpoint_000000)\n"
     ]
    }
   ],
   "source": [
    "tune_results = tuner.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "15e1df83-b00a-4f0a-8f01-6070c46d71bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = tune_results[0].checkpoint.path + \"/checkpoint\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(path, num_labels=3).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7ebae5ec-5d3b-43ce-b390-9dc7e58eb6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7353/7353 [00:34<00:00, 212.56it/s]\n"
     ]
    }
   ],
   "source": [
    "y_pred = []\n",
    "y_true = []\n",
    "for example in tqdm(test_dataset):\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(example['text'], return_tensors=\"pt\")\n",
    "        logits = model(input_ids=inputs['input_ids'].cuda()[:,:512], \n",
    "                      attention_mask= inputs['attention_mask'].cuda()[:,:512]).logits\n",
    "        prediction = logits.argmax().item()\n",
    "        y_true.append(example['label'])\n",
    "        y_pred.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "bc1eb877-0f17-4336-afce-1d1a00c953a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.72418478, 0.65909677, 0.36818409])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = f1_score(y_pred, y_true, average=None)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6ea36911-4629-461e-a779-b71272cde8c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5838218829494224"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc26d01-84bc-4408-849a-54de0b6a3d85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
